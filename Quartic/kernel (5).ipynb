{
  "cells": [
    {
      "metadata": {
        "_uuid": "a193ea101c688daf03b0853ead3f12ad8375eb0a"
      },
      "cell_type": "markdown",
      "source": "## Objective\nHave to build the most accurate model which can predict target column for data_test.csv. \n\n1. The column details are below:\n> * id: id column for data_train, data_test, respectively\n> * num*: numerical features\n> * der*: derived features from other features\n> * cat*: categorical features\n> * target: target column, only exists in data_train. it is binary.\n2. There are potentially missing values in each column. The goal is to predict target column for data_test.csv.The solution should have a result csv file with two columns:\n> * 'id': the id column from data_test.csv\n> * 'target': the predicted probability of target being 1"
    },
    {
      "metadata": {
        "_uuid": "011bcf724719258b030f9e5b6f745bfb0d12e7be"
      },
      "cell_type": "markdown",
      "source": "## Index"
    },
    {
      "metadata": {
        "_uuid": "0e2f3d1f32db65a058daaf5fcf302de6d023c8eb"
      },
      "cell_type": "markdown",
      "source": "* <a href='#section1'>Reading Data</a>\n* <a href='#section2'>Data Cleaning</a>\n* <a href='#section3'>Exploration</a>\n* <a href='#section4'>Feature Generation</a>\n* <a href='#section5'>Generic Methods</a>\n\n* <a href='#section6'>Plot Class Distribution</a>\n* <a href='#section7'>Cross Validation Model Evaluation</a>\n* <a href='#section8'>Feature importance</a>\n* <a href='#section9'>Train Test Distribution</a>\n\n* <a href='#section10'>Plot Class Distribution</a>\n* <a href='#section11'>UnSupervised Feature Interaction</a>\n* <a href='#section12'>Modelling on Dae Data</a>\n* <a href='#section13'>Model Tuning</a>\n\n* <a href='#section14'>Discussion Questions Answers</a>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba26cf58a86ef14b75307570536aecc4fbf8ae99"
      },
      "cell_type": "code",
      "source": "%%time\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport random\nimport xgboost as xgb\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom math import sqrt\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 82.1 ms\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2c85fb56b5f63d6875d87c64f26f24305c0ad71"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "027533c78e35c199d2a66e54d98ba6b60e5b50d3"
      },
      "cell_type": "code",
      "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.models import load_model\nfrom keras.initializers import glorot_normal, Zeros, Ones\nimport keras.backend as K\nfrom keras.optimizers import RMSprop\nimport tensorflow as tf",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6922fc6a796f2d8e0df14e98bdd49d9f09ab735b"
      },
      "cell_type": "code",
      "source": "from IPython.core.interactiveshell import InteractiveShell\nfrom tqdm import tqdm_notebook\nInteractiveShell.ast_node_interactivity = \"all\"",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7aca0f2c6b5cef4c12fb67bf90e36720be1b3b3d"
      },
      "cell_type": "markdown",
      "source": "<a id ='section1'></a>"
    },
    {
      "metadata": {
        "_uuid": "069ed7461942013f365fb5cfd1e8e3298ee2dc5f"
      },
      "cell_type": "markdown",
      "source": "### Reading & understanding Data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../input/ds_data_big/ds_data\"))\ntrain = pd.read_csv('../input/ds_data_big/ds_data/data_train.csv')\ntest = pd.read_csv('../input/ds_data_big/ds_data/data_test.csv')",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['readme', 'data_train.csv', 'data_test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "181ddc3f81fa419ea2af6b7e4657aec4a07b1001",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9fadce6fc19d6add9c4a930796aa3c73cda81271"
      },
      "cell_type": "code",
      "source": "train.shape\ntest.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3af2977061f811adc1282a0ec9975eadde99371f"
      },
      "cell_type": "markdown",
      "source": "* 56 Anonymous Features in both train and test except id & target\n* These Features are Numerical,Derived & categorical"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24a780ae18ff591290ff1cc0c86ba06233af58cc"
      },
      "cell_type": "code",
      "source": "train.describe()\ntest.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdab1e447af2088d96f5c1c62d3b4a770277470b"
      },
      "cell_type": "markdown",
      "source": "* Feature num3-10 have almost identical std ,mean,unique values and distribution\n* Feature der1,2,3 also have almost identical std ,mean,unique values and distribution.\n* Both train test seem to have same distribution"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ef05377db50976990905f644b0e5eafaf25d95e"
      },
      "cell_type": "code",
      "source": "print('class count in numbers: ')\ntrain['target'].value_counts()\nprint('percentage of class count : ')\ntrain['target'].value_counts()/train.shape[0] * 100",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ac00ec307733e8c1bfe5d5e8de8a6512f126614"
      },
      "cell_type": "markdown",
      "source": "* Unbalanced Dataset with minor class have only 3.6% of contribution\n* So,The metric used for the classification is Roc_Auc which take care both precision & recall for 2 classes"
    },
    {
      "metadata": {
        "_uuid": "6b90d3de42ded52af6b58c8e8e4c7c80827f7a88"
      },
      "cell_type": "markdown",
      "source": "<a id ='section2'></a>"
    },
    {
      "metadata": {
        "_uuid": "d5bc8aaaa4a144a90be4f323747ca2513c8cc1f3"
      },
      "cell_type": "markdown",
      "source": "### DataCleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "57e531a24d25e2f471b74e59bcc30d484c663ae1"
      },
      "cell_type": "code",
      "source": "##get duplicated rows\ntrain[\"is_duplicate\"]= train[list(set(train.columns) - set(['id']))].duplicated()",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d327865331962b36edb8f7c22c1780ccf8ecfea"
      },
      "cell_type": "code",
      "source": "##788 duplicated rows found\ntrain.groupby(['is_duplicate'])['is_duplicate'].count()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "is_duplicate\nFalse    595212\nTrue        788\nName: is_duplicate, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e2db0a606ecc762a9e99e69c2563e85f295618d"
      },
      "cell_type": "code",
      "source": "train.drop(train[train['is_duplicate'] == True].index, inplace=True)\ntrain.drop(['is_duplicate'],axis=1,inplace=True)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9d1e077ac96acb422137c56fb211c61ff9ee8fa2"
      },
      "cell_type": "markdown",
      "source": "* Removed 788 rows from the train dataset.\n* Test rows doesn't have any duplicates."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f3ed5fd7e8d40d67256c0146d7fec2be63e70ba"
      },
      "cell_type": "code",
      "source": "train_enc =  pd.DataFrame(index = train.index)\nfor col in tqdm_notebook(train.columns):\n    train_enc[col] = train[col].factorize()[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db9a19ef6d43de1d4fce94e21e04f1c90788c53b"
      },
      "cell_type": "code",
      "source": "dup_cols = {}\nfor i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n    for c2 in train_enc.columns[i + 1:]:\n        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n            dup_cols[c2] = c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81b6920e99aabd348c91f06a65ea5f287c6de070"
      },
      "cell_type": "code",
      "source": "dup_cols",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "138a4bbbeb782e8d8ed6115ac840f79ebc129551"
      },
      "cell_type": "markdown",
      "source": "* There are no duplicate Columns"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1da56d59f54494cb94c60a5263a8c604c55402ae"
      },
      "cell_type": "code",
      "source": "#making copy\ntrn =  train.copy()\ntst =  test.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ea75385180887536ad336cc10798373858c8678"
      },
      "cell_type": "code",
      "source": "#combining test and train data\ndf_combine = pd.concat([trn, tst], axis=0, ignore_index=True)\n#dropping 'target' column as it is not present in the test\ndf_combine = df_combine.drop('target', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b214f8b567f7d75cca28bcab8a9c6c146344fda"
      },
      "cell_type": "code",
      "source": "df_combine = df_combine.drop('id', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6700cd47e521f532a4f37f608b8c9b7c438d909c"
      },
      "cell_type": "code",
      "source": "df_combine[\"is_duplicate\"]= df_combine.duplicated()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07672f0079d580156093e9b53abe3a079f084562"
      },
      "cell_type": "code",
      "source": "df_combine[\"is_duplicate\"].nunique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95365aec12e5d6f842bda6b712ce2dbb1f489d7e"
      },
      "cell_type": "code",
      "source": "df_combine.groupby(['is_duplicate'])['is_duplicate'].count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "728c3e087a11c365b0da79d991139fae12ed52d8"
      },
      "cell_type": "code",
      "source": "df_combine.drop(df_combine[df_combine['is_duplicate'] == True].index, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "be06d7b5b3c4b8b92e212fc2185071e4285880be"
      },
      "cell_type": "markdown",
      "source": "* To check Either any train rows present in test set or not."
    },
    {
      "metadata": {
        "_uuid": "d293b413f2e62379992b6e99db007ab658bbd977"
      },
      "cell_type": "markdown",
      "source": "<a id ='section3'></a>"
    },
    {
      "metadata": {
        "_uuid": "dee21fbdc45cc7023c5e8e73ff4944b4e89da249"
      },
      "cell_type": "markdown",
      "source": "### Exploration"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d5d3f308cc5bb18b0ff9aed70bf107bc16a450f"
      },
      "cell_type": "code",
      "source": "# Number of NaNs in each rows\ntrain.isnull().sum(axis=1).head(20)\ntest.isnull().sum(axis=1).head(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c316971a9af61f242384e818b4edf08754527abd"
      },
      "cell_type": "code",
      "source": "#Type of data in train\ntrain.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56cb35dcc65a23c15dc23263d5205868f7b271a3"
      },
      "cell_type": "code",
      "source": "# Number of NaNs in each column\ntrain.isnull().sum(axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7599b8009f96544c5180e1901ad1e08e69b953c1"
      },
      "cell_type": "code",
      "source": "train.nunique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c06596619954a95bed18b6d474b57a0d22aa0cbf"
      },
      "cell_type": "markdown",
      "source": "* Data Type are int64,float64\n* Columns like num18,num22,cat1,cat2,cat3,cat4,cat5,cat6,cat8,cat10,cat12 have Nan Values\n* Columns like num18,num21,num20,num22,cat14 have high Cardinality"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39fbbe4f49f7a0b01be76d1dbb52bc3b605999db"
      },
      "cell_type": "code",
      "source": "##these numeric features have higher mean value for label 1 as compared to label 0.\ntrain[['num18', 'num21','cat14','num20','target']].groupby(['target']).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41354fd7a1d701a1fb315e7a12d050f43a63f8a5"
      },
      "cell_type": "code",
      "source": "num_cols = [c for c in train.columns if c.startswith('num') or c.startswith('target')]\nder_cols = [c for c in train.columns if c.startswith('der')]\ncat_cols = [c for c in train.columns if c.startswith('cat') or c.startswith('target')]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3691d6d65b47032eae39ab3aae2256d6e078ddd"
      },
      "cell_type": "code",
      "source": "f,ax=plt.subplots(figsize=(12,12))\nsns.heatmap(train[cat_cols].corr(),annot=True,linewidths=.5,fmt='.1f',ax=ax)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e99571703ae8aca69e657d5c85431726368d61c"
      },
      "cell_type": "markdown",
      "source": "* Features num18,num20,num21,num22 have higher correlation with Target value"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e572a164418f5cc74afeec3efd08abd35c0904b9"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(10, 8))\ntrain_1= train[train['target']==0]\nsns.distplot(train_1['num21'], kde=True, hist=True, norm_hist=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "416a403b6fabf1ed0b165659081aabd02d960fa7"
      },
      "cell_type": "code",
      "source": "# train[train['cat14'] == 104].shape\ntrain[train['num22'] <= 0.28].shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2c94a668fdb54a2f43b82216c4a8d457381918f"
      },
      "cell_type": "code",
      "source": "# train.iloc[:,:1]\ntrain['num22'].value_counts().sort_index().plot.area()\n# sns.countplot(train['num21'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81d63b82fb04942f1b355f98748c41db343aff64"
      },
      "cell_type": "code",
      "source": "test[test['num21']>3.0].shape\ntrain[train['num21']>3.0].groupby(['target'])['target'].count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "bdca5e0a21482bc2ec0dad9295705385593a3c64"
      },
      "cell_type": "code",
      "source": "train[train['num22'] < 0.28].groupby(['target'])['target'].count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90683e87efc3b3cadfa4d61018b763117d9e625e"
      },
      "cell_type": "code",
      "source": "# train['target'].value_counts().sort_index().plot.bar()\n# train['num2'].value_counts().plot.area()\nsns.countplot(train['der14'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd812ef247701e0c3143d98a106a1002671e3844"
      },
      "cell_type": "code",
      "source": "##can assign the each chart to one axes at a time\nfig,axrr=plt.subplots(2,2,figsize=(14,8))\n\nsns.countplot(train['num1'],ax=axrr[0][0])\nsns.countplot(train['num2'],ax=axrr[0][1])\nsns.countplot(train['num3'],ax=axrr[1][0])\nsns.countplot(train['num4'],ax=axrr[1][1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eafcfc581bc9ceb0516a828d9461bb722d69beec"
      },
      "cell_type": "markdown",
      "source": "<a id ='section4'></a>"
    },
    {
      "metadata": {
        "_uuid": "b1b924710e451358b4bf60ff81fabfbfe000b8ca"
      },
      "cell_type": "markdown",
      "source": "### Feature Generation by Feature Interaction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6884ef037a8a9cf249c5a29edac10e2ff46bd46d"
      },
      "cell_type": "code",
      "source": "col = ['id', 'num1', 'num2', 'num3', 'num4', 'num5', 'num6', 'num7', 'num8','num9', 'num10', 'num11', 'num12', 'num13', 'num14', 'num15', 'num16',\n       'num17', 'num18', 'num19', 'num20', 'num21', 'num22', 'num23', 'der1','der2', 'der3', 'der4', 'der5', 'der6', 'der7', 'der8', 'der9', 'der10',\n       'der11', 'der12', 'der13', 'der14', 'der15', 'der16', 'der17', 'der18','der19', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14']",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3bddefaa4000f676e029640cc6499c5cd809b830"
      },
      "cell_type": "markdown",
      "source": "* These Features have num18,num20,num21,num22,cat14 higher importance As we find latter in the Notebook"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ce9c8b8fab3401030de1ecee5d4dcdb48b161b4"
      },
      "cell_type": "code",
      "source": "train['sum_zero'] = (train[col] == 0).sum(axis=1)\ntest['sum_zero'] = (test[col] == 0).sum(axis=1)\n\ntrain['num18m21'] = train['num18'] * train['num21']\ntest['num18m21'] = test['num18'] * test['num21']\ntrain['num18a21'] = train['num18'] + train['num21']\ntest['num18a21'] = test['num18'] + test['num21']",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c4da1d59c67a3b2a8dbf81c60f1a54b7d500ee7"
      },
      "cell_type": "code",
      "source": "##Replace all Nan by Mode\nfor i in train.columns:\n    train[i].fillna(train[i].mode()[0], inplace=True)\nfor i in test.columns:\n    test[i].fillna(test[i].mode()[0], inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2fe631e678e753cac33fce39904fb233e95baeef"
      },
      "cell_type": "markdown",
      "source": "* Split the data into train_id, train, target & test_id, test"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "587a6be33cff360e9e7d6d7d0bce2d2f0c811198"
      },
      "cell_type": "code",
      "source": "Y1=train['target']\n# train1=train.drop(['employee_id','is_promoted'],axis=1)\ntrain1=train.drop(['id','target'],axis=1)\n# train1=train.drop(['id','num7','num8','num9','num10','num11','cat10','cat13','target'],axis=1)\ntrain1=train1\nY=Y1.values\n\ntest_id=test['id']\n# test1 = test.drop(['employee_id'],axis=1)\n# test1 = test.drop(['id','num7','num8','num9','num10','num11','cat10','cat13'],axis=1)\ntest1 = test.drop(['id'],axis=1)\ntest1=test1",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f5b55b4e3ecda46f9b26233038c3652ad3125b9"
      },
      "cell_type": "markdown",
      "source": "<a id ='section5'></a>"
    },
    {
      "metadata": {
        "_uuid": "155ecde8e82ac54f5ff5af0b6a9ca25546c27c4d"
      },
      "cell_type": "markdown",
      "source": "### Generic Methods\n* Function to plot the area under the curve (AUC) for a model & Plotting false positive rate and true positive rate\n* Model evaluation Methods for classification_report"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bf9ae6ae5d3868370b384c7b626e21d12d502e0"
      },
      "cell_type": "code",
      "source": "## This is a generic function to plot the area under the curve (AUC) for a model\ndef plot_auc(y_test,y_pred):\n    fp_rate, tp_rate, treshold = roc_curve(y_test, y_pred)\n    auc_score = auc(fp_rate, tp_rate)\n    plt.figure()\n    plt.title('ROC Curve')\n    plt.plot(fp_rate, tp_rate, 'b', label = 'AUC = %0.2f' % auc_score)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "937f579730ea48b6770161f9ddfa521e87e55f3a"
      },
      "cell_type": "code",
      "source": "def evaluate_model(model, X_test_param, y_test_param):\n    print(\"Model evaluation\")\n#     X_test_param, X_test, y_test_param, y_test = train_test_split(X_test_param, y_test_param, random_state=42,test_size=0.1)\n#     model.fit(X_test_param, y_test_param,eval_set=[(X_test, y_test)],early_stopping_rounds=50,verbose=50)\n    model.fit(X_test_param, y_test_param)\n    y_pred = model.predict_proba(X_test_param)[:, 1]\n    print(\"Accuracy: {:.5f}\".format(model.score(X_test_param, y_test_param)))\n    print(\"AUC: {:.5f}\".format(roc_auc_score(y_test_param, y_pred)))\n    print(\"\\n#### Classification Report ####\\n\")\n    \n    thresholds = np.linspace(0.01, 0.99, 50)\n    mcc = np.array([f1_score(y_test_param, y_pred>thr) for thr in thresholds])\n    best_threshold = thresholds[mcc.argmax()]\n    predictions = list(map(lambda x: 1 if x > best_threshold else 0,y_pred))\n    print(classification_report(y_test_param, predictions, target_names=['0','1']))\n    plot_auc(y_test_param, y_pred )",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e56b362e8c797fcd772b93f92607882dde25e2b8"
      },
      "cell_type": "code",
      "source": "train1.shape\ntest1.shape\nY.shape",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "(595212, 59)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "(892816, 59)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "(595212,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4db95c39df2245a385db868458c60874b151047a"
      },
      "cell_type": "markdown",
      "source": "<a id ='section6'></a>"
    },
    {
      "metadata": {
        "_uuid": "fd14b765fc38ca3b1112dc4e9881354a90697d7d"
      },
      "cell_type": "markdown",
      "source": "### Plot Class Distribution"
    },
    {
      "metadata": {
        "_uuid": "9fff145c060fdfb837413f6f577ff99e44696330"
      },
      "cell_type": "markdown",
      "source": "* Scale before applying PCA or Linear regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b3e854bd1e808b4bfe79ae42bb3a881c4c3ff68"
      },
      "cell_type": "code",
      "source": "scaler = StandardScaler()\n# scaler = QuantileTransformer(n_quantiles=10, random_state=0)\nscaler.fit(train1)\n# Apply transform to both the training set and the test set.\ntrain2 = scaler.transform(train1)\ntest2 = scaler.transform(test1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a5ca57dfa8f649c14df9e27cc14bbccbb7b795a5"
      },
      "cell_type": "markdown",
      "source": "> PCA to find the overlap between the decision boundaries of 1 and 0"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43399318f1c8b967311366481edb4556e98c4f13"
      },
      "cell_type": "code",
      "source": "pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(train2)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n\nfinalDf = pd.concat([principalDf, train['target']], axis = 1)\nfinalDf.head(5)\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\n\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\nax.legend(targets)\nax.grid()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a46e5819e87f5e678ae7e92df4e98409cb1ea448"
      },
      "cell_type": "markdown",
      "source": "> Decision Boundary between 0 and 1 are overlapping in two dimensions.So Linear model will not perform well"
    },
    {
      "metadata": {
        "_uuid": "6b99f75057c7ad34bc39ba97008910fc5e811451"
      },
      "cell_type": "markdown",
      "source": "<a id ='section7'></a>"
    },
    {
      "metadata": {
        "_uuid": "7ea4066dd9e98143d0a9da0e067ba389114e60ac"
      },
      "cell_type": "markdown",
      "source": "### Model Evaluation & Cross Validation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cd72c50d576afe06ce6b7b009479cfc3c1a14b7"
      },
      "cell_type": "code",
      "source": "clf = lgb.LGBMClassifier()\n# clf = LogisticRegression()\nevaluate_model(clf, train1.values, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc6b05a75a0c3f4b0ffd4dfdad3f511a7b9bda56"
      },
      "cell_type": "code",
      "source": "#create the cross validation fold for different boosting and linear model.\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nSEED=42\nst_train = train1.values\nst_test = test1.values\nclf1 = xgb.XGBClassifier()\nclf2 = lgb.LGBMClassifier(max_depth= 8, learning_rate=0.0941, n_estimators=197, num_leaves= 17, reg_alpha=3.4492 , reg_lambda= 0.0422) #lgb_pca\nclf = ensemble.VotingClassifier(estimators = [('clf1', clf1),('clf2', clf2)],\n                                       voting = 'soft', weights = [3,3])\n\nfold = 3\ncv = StratifiedKFold(Y, n_folds=fold,shuffle=True, random_state=30)\nX_preds = np.zeros(st_train.shape[0])\npreds = np.zeros(st_test.shape[0])\nfor i, (tr, ts) in enumerate(cv):\n    print(ts.shape)\n    mod = clf.fit(st_train[tr], Y[tr])\n    X_preds[ts] = mod.predict_proba(st_train[ts])[:,1]\n    preds += mod.predict_proba(st_test)[:,1]\n    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(Y[ts], X_preds[ts])))\nscore = roc_auc_score(Y, X_preds)\nprint(score)\npreds1 = preds/fold",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(198405,)\nfold 0, ROC AUC: 0.636\n(198404,)\nfold 1, ROC AUC: 0.643\n(198403,)\nfold 2, ROC AUC: 0.636\n0.6382320382113563\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7a75b718c792d55032902874d2c18910b4c84435"
      },
      "cell_type": "markdown",
      "source": "> Ensemble of Xgboost and Lightgbm using VotingClassifier gave Cross validation cv of 64%"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "434c570637f93051f72954701db739225b501741"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame({'id': test_id, 'target': preds1})\nsub=sub.reindex(columns=[\"id\",\"target\"])\nsub.to_csv('sub_vot.csv', index=False)\n\nsub_oof = pd.DataFrame({'id': train['id'], 'target': X_preds})\nsub_oof=sub_oof.reindex(columns=[\"id\",\"target\"])\nsub_oof.to_csv('sub_oof_vot.csv', index=False)",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56758a206aceb78e8ec16b88f974f0fdeb091179"
      },
      "cell_type": "code",
      "source": "#lgb.plot_importance(clf,figsize=(20,10))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ff9a8f230afb405e6d8c390c423660c7a886815"
      },
      "cell_type": "markdown",
      "source": "<a id ='section8'></a>"
    },
    {
      "metadata": {
        "_uuid": "6e38b8a7aea921f569944f01447d3d13e3013c71"
      },
      "cell_type": "markdown",
      "source": "### Feature importance"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94ac3174a4f33165dd8a8e02cdaf6ea90007cc93"
      },
      "cell_type": "code",
      "source": "rf = RandomForestClassifier()\nrf.fit(train1,Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b42e7ea22dfc49b0a63d715489fbfa48b7e69582"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(20,10))\nplt.plot(rf.feature_importances_)\n\nplt.xticks(np.arange(train1.shape[1]), train1.columns.tolist(), rotation=90);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9436d38118b5f31b96a2b7c1f59047e0468b30c"
      },
      "cell_type": "markdown",
      "source": ">Permutation Feature Importance\n\n>RandomForest Feature Importance"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff294f0fc4587e2bdc05612e2c6362effde123f6"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport eli5\nfrom eli5.sklearn import PermutationImportance\ntrain_X, val_X, train_y, val_y = train_test_split(train1[:100000], Y[:100000], random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)\n\n#Make a small change to the code below to use in this problem. \nperm = PermutationImportance(rf, random_state=1).fit(val_X, val_y)\n\n#uncomment the following line to visualize your results\neli5.show_weights(perm, feature_names = val_X.columns.tolist())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b265b718d5784232824768787b00ff77ffe9969"
      },
      "cell_type": "markdown",
      "source": "<a id ='section9'></a>"
    },
    {
      "metadata": {
        "_uuid": "4433e2d940c50d68fa5996dd5af508f10b025989"
      },
      "cell_type": "markdown",
      "source": "### Train Test Distribution\nTo check Whether the Train and test have same Distribution or not"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98896ace73b981c9cfb278668248cf2585ca405f"
      },
      "cell_type": "code",
      "source": "#making copy\ntrn =  train.copy()\ntst =  test.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2f213e26bf73bb10c4999de59fb69e97bd559bc"
      },
      "cell_type": "code",
      "source": "tst['is_train'] = 0\ntrn['is_train'] = 1 #1 for train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54f3947135b373ff8ce7c673c4d1f29dc03145e9"
      },
      "cell_type": "code",
      "source": "#combining test and train data\ndf_combine = pd.concat([trn, tst], axis=0, ignore_index=True)\n#dropping 'target' column as it is not present in the test\ndf_combine = df_combine.drop('target', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2494a1b5323dc8d42ccbbb678c9e0448cce2c764"
      },
      "cell_type": "code",
      "source": "y = df_combine['is_train'].values #labels\nx = df_combine.drop('is_train', axis=1).values #covariates or our dependent variables",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f0132d19c8645bfbb1452fbda4c526a3e8459d9"
      },
      "cell_type": "code",
      "source": "scaler = StandardScaler()\n# scaler = QuantileTransformer(n_quantiles=10, random_state=0)\nscaler.fit(x)\n# Apply transform to both the training set and the test set.\ntrain2 = scaler.transform(x)\n# test2 = scaler.transform(test1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2dbb5d815b045343f6e0f82ef063056a9b66ae9f"
      },
      "cell_type": "markdown",
      "source": "<a id ='section10'></a>"
    },
    {
      "metadata": {
        "_uuid": "7e164ee5aebe40e0017fafebde061305fcb89b68"
      },
      "cell_type": "markdown",
      "source": "### Plot Class Distribution"
    },
    {
      "metadata": {
        "_uuid": "32db00220186196abe13519b95d73efe25e8f1e4"
      },
      "cell_type": "markdown",
      "source": ">Other than the use of Predictive Model to find Distribution similarity.\n\n>Can also use PCA for Finding Overlapping train and test sets on 2 dimensions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72b21792bee7881d629e6d13bb4dea7e76085837"
      },
      "cell_type": "code",
      "source": "pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(train2)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n\nfinalDf = pd.concat([principalDf, df_combine['is_train']], axis = 1)\nfinalDf.head(5)\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\n\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['is_train'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e0394b81f6eeb6d71e0c7e0369ee5ab164f1644"
      },
      "cell_type": "markdown",
      "source": "> Both Train Test have Same distribution.So,Results on train set will be equal to test set"
    },
    {
      "metadata": {
        "_uuid": "3ce43ead4bdaa86f09670f66361c5d4cda41075b"
      },
      "cell_type": "markdown",
      "source": "<a id ='section11'></a>"
    },
    {
      "metadata": {
        "_uuid": "8fcacb7d824d583acc058aebda39795d7a88fe76"
      },
      "cell_type": "markdown",
      "source": "### UnSupervised Feature Interaction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5cb1fdc244c8f6d81545ebc32917b57cb9e326f"
      },
      "cell_type": "code",
      "source": "df_train = train.copy()\ndf_test = test.copy()\ntrain_target = train['target'].values\nntrain = df_train.shape[0]\nntest  = df_test.shape[0]",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a2b2fe6a5f1afdb8a252181c08baf9d2806c238"
      },
      "cell_type": "code",
      "source": "from scipy.special import erfinv\ndef hot_encoder(df, columns):\n    one_hot = {c: list(df[c].unique()) for c in columns}\n    for c in one_hot:\n        for val in one_hot[c]:\n            df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n    return df\n\ndef scale_feat(df):\n    scaler = StandardScaler()\n    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\ndef rank_gauss(x):\n    # x is numpy vector\n    N = x.shape[0]\n    temp = x.argsort()\n    rank_x = temp.argsort() / N\n    rank_x -= rank_x.mean()\n    rank_x *= 2 # rank_x.max(), rank_x.min() should be in (-1, 1)\n    efi_x = erfinv(rank_x) # np.sqrt(2)*erfinv(rank_x)\n    efi_x -= efi_x.mean()\n    return efi_x\n\ndef df_inputSwapNoise(df, p):\n    ### feature with another value from the same column with probability p\n    n = df.shape[0]\n    idx = list(range(n))\n    swap_n = round(n*p)\n    for col in df.columns:\n        arr = df[col].values\n        col_vals = np.random.permutation(arr)\n        swap_idx = np.random.choice(idx, size= swap_n)\n        arr[swap_idx] = np.random.choice(col_vals, size = swap_n)\n        df[col] = arr\n    return df",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b3e480e899a5b2d16c3e534dd01dbe8a0d43d41"
      },
      "cell_type": "code",
      "source": "print('Transforming data')\nfeature_cols = [c for c in df_train.columns if c not in ['id','target']]\nkeep_cols    = [c for c in feature_cols]\n#scale_cols = ['num18','num20','num21','num22','cat14']\n#keep_cols    = [c for c in feature_cols if c not in scale_cols] \n#cat_cols     = [c for c in keep_cols if '_cat' in c]\n##num18,num20,num21,num22,cat14\n\ndf_all = pd.concat([df_train[keep_cols], df_test[keep_cols]])\n#df_all = scale_feat(df_all)\ndf_all_org = df_all.copy()\ndf_all_noise = df_inputSwapNoise(df_all, 0.15)\n#df_all = hot_encoder(df_all, keep_cols)\ndata_all_org = df_all_org.values\ndata_all_noise = df_all_noise.values\ncols = data_all_org.shape[1]\n#print(df_all.columns)\nprint('Final data with {} columns'.format(cols))",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Transforming data\nFinal data with 59 columns\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c9ff9f5ab1585fd450d60773de2391b1953386f"
      },
      "cell_type": "code",
      "source": "for i in range(cols):\n    u = np.unique(data_all_org[:,i])\n    if u.shape[0] > 3:\n        data_all_org[:,i] = rank_gauss(data_all_org[:,i])\n\nfor i in range(cols):\n    u = np.unique(data_all_noise[:,i])\n    if u.shape[0] > 3:\n        data_all_noise[:,i] = rank_gauss(data_all_noise[:,i])\n\ntrain_data_orig = data_all_org[0:ntrain,:]\ntest_data_orig  = data_all_org[ntrain:,:]\ntrain_data_noise = data_all_noise[0:ntrain,:]\ntest_data_noise  = data_all_noise[ntrain:,:]\nprint(train_data_orig.shape)\nprint(test_data_orig.shape)\nprint(train_data_noise.shape)\nprint(test_data_noise.shape)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(595212, 59)\n(892816, 59)\n(595212, 59)\n(892816, 59)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4b4d0fd1257b00bb4eff1d87468d233ef0e3b36"
      },
      "cell_type": "code",
      "source": "print('Original data')\nall_data = np.vstack((train_data_orig, test_data_orig))\nprint('Noise data')\nall_data_noise = np.vstack((train_data_noise, test_data_noise))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Original data\nNoise data\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4df0c4e69988a76155f39f790573fe6add70a842"
      },
      "cell_type": "code",
      "source": "print('Creating neural net')\nmodel = Sequential()\nmodel.add(Dense(units=1500, input_dim = all_data.shape[1], kernel_initializer=glorot_normal()))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=1500, kernel_initializer=glorot_normal()))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=1500, kernel_initializer=glorot_normal()))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(all_data.shape[1])) \nmodel.add(Activation('linear'))\n\nopt = keras.optimizers.Adam(lr=0.001)\nmodel.compile(loss='mse', optimizer=opt)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating neural net\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "086eb70469ce571b769f467ce4f8c89a0c8deadf"
      },
      "cell_type": "code",
      "source": "print('Training neural net')\nepochs = 5\nchck = ModelCheckpoint('keras_dae.h5', monitor='loss', save_best_only=True)\ncb = [ EarlyStopping(monitor='loss', patience=100, verbose=2, min_delta=0), chck ]\nmodel.fit(all_data_noise, all_data, batch_size=128, verbose=1, epochs=epochs, callbacks=cb)\n\nprint('Applying neural net')\ntrain_data_transform = model.predict(train_data_orig)\ntest_data_transform = model.predict(test_data_orig)\nprint(train_data_transform.shape)\nprint(test_data_transform.shape)",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training neural net\nEpoch 1/5\n1488028/1488028 [==============================] - 185s 125us/step - loss: 0.1083\nEpoch 2/5\n1488028/1488028 [==============================] - 183s 123us/step - loss: 0.0863\nEpoch 3/5\n1488028/1488028 [==============================] - 183s 123us/step - loss: 0.0819\nEpoch 4/5\n1488028/1488028 [==============================] - 183s 123us/step - loss: 0.0798\nEpoch 5/5\n1488028/1488028 [==============================] - 183s 123us/step - loss: 0.0785\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f7e10bc3ba8>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "Applying neural net\n(595212, 59)\n(892816, 59)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2898d69250d7b72645cfad87b94ad4910122df1"
      },
      "cell_type": "code",
      "source": "train_data_transform.shape\ntest_data_transform.shape",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "(595212, 59)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "(892816, 59)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "35916bf76e6e1b8d1c1b8f9c75e64b04d81bff7a"
      },
      "cell_type": "markdown",
      "source": "<a id ='section12'></a>"
    },
    {
      "metadata": {
        "_uuid": "b575633c358cd9840ff7e56b1aa7588ddd866dfd"
      },
      "cell_type": "markdown",
      "source": "### Modelling on Dae Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87d8b224cbe47a850d4839625f989e0f9c11fe14"
      },
      "cell_type": "code",
      "source": "clf = lgb.LGBMClassifier()\n# clf = lgb.LGBMClassifier(objective = 'binary',metric='binary_logloss',max_depth= 8, learning_rate=0.0941, n_estimators=197, num_leaves= 17, reg_alpha=3.4492 , reg_lambda= 0.0422)\nevaluate_model(clf, train_data_transform, train_target)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd5487a88a51ca5f81c6a04d2604cd48c8397cba"
      },
      "cell_type": "code",
      "source": "#create the cross validation fold for different boosting and linear model.\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nSEED=42\nst_train = train_data_transform\nst_test = test_data_transform\nY = train_target\nclf1 = xgb.XGBClassifier()\nclf2 = lgb.LGBMClassifier(max_depth= 8, learning_rate=0.0941, n_estimators=197, num_leaves= 17, reg_alpha=3.4492 , reg_lambda= 0.0422) #lgb_pca\nclf = ensemble.VotingClassifier(estimators = [('clf1', clf1),('clf2', clf2)],\n                                       voting = 'soft', weights = [3,3])\n\nfold = 3\ncv = StratifiedKFold(Y, n_folds=fold,shuffle=True, random_state=30)\nX_preds = np.zeros(st_train.shape[0])\npreds = np.zeros(st_test.shape[0])\nfor i, (tr, ts) in enumerate(cv):\n    print(ts.shape)\n    mod = clf.fit(st_train[tr], Y[tr])\n    X_preds[ts] = mod.predict_proba(st_train[ts])[:,1]\n    preds += mod.predict_proba(st_test)[:,1]\n    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(Y[ts], X_preds[ts])))\nscore = roc_auc_score(Y, X_preds)\nprint(score)\npreds1 = preds/fold",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(198405,)\nfold 0, ROC AUC: 0.631\n(198404,)\nfold 1, ROC AUC: 0.636\n(198403,)\nfold 2, ROC AUC: 0.629\n0.6317488511343456\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0eaaa5b2e9ccf9e831b3db2e6e0463c3d2cb4c08"
      },
      "cell_type": "code",
      "source": "sub1 = pd.DataFrame({'id': test_id, 'target': preds1})\nsub1=sub1.reindex(columns=[\"id\",\"target\"])\nsub1.to_csv('sub_dae.csv', index=False)\n\nsub1_oof = pd.DataFrame({'id': train['id'], 'target': X_preds})\nsub1_oof=sub1_oof.reindex(columns=[\"id\",\"target\"])\nsub1_oof.to_csv('sub_oof_dae.csv', index=False)",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "429aa478117dd6ed799307c9b6f65a0d111decb0"
      },
      "cell_type": "markdown",
      "source": "### Weighted Average"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6224efa00775bd7a27ba02a5ac861b7241b3a758"
      },
      "cell_type": "code",
      "source": "targ = 0.65 * sub_oof['target'] + 0.35 * sub1_oof['target']\ntarg1 = 0.65 * sub['target'] + 0.35 * sub1['target']",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7f611d3c0f135543fb667bb861e306456fe187e"
      },
      "cell_type": "code",
      "source": "roc_auc_score(Y, targ.values)",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "0.6379401234597274"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66699a688d2d7390149eae1ba71e1e8732cea6de"
      },
      "cell_type": "code",
      "source": "subfin = pd.DataFrame({'id': test_id, 'target': targ1})\nsubfin=subfin.reindex(columns=[\"id\",\"target\"])\nsubfin.to_csv('sub_final.csv', index=False)",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d660bc7632af6ee9d5c35912c0d11b427f056f2"
      },
      "cell_type": "code",
      "source": "# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"sub_final.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(subfin)\n#     Yay, download link!    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c41d1e5a0982de99263bc0408309444f4c30fe5f"
      },
      "cell_type": "markdown",
      "source": "<a id ='section13'></a>"
    },
    {
      "metadata": {
        "_uuid": "e1f0eaf65c5e6bb7f36e457934e92dc250372794"
      },
      "cell_type": "markdown",
      "source": "### LightGbm Model Tuning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d80eda15364288723b89b9a6b7483bcc007659e7"
      },
      "cell_type": "code",
      "source": "#lightgbm bayesian optimization\nfrom sklearn.cross_validation import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\ndef xgboostcv(max_depth,learning_rate,n_estimators,num_leaves,reg_alpha,reg_lambda):\n    return cross_val_score(lgb.LGBMClassifier(max_depth=int(max_depth),learning_rate=learning_rate,n_estimators=int(n_estimators),\n                                             silent=True,nthread=-1,num_leaves=int(num_leaves),reg_alpha=reg_alpha,\n                                           reg_lambda=reg_lambda),train1,Y,\"roc_auc\",cv=3).mean()\n\nxgboostBO = BayesianOptimization(xgboostcv,{'max_depth': (3, 10),'learning_rate': (0.001, 0.1),'n_estimators': (10, 1000),\n                                  'num_leaves': (4,30),'reg_alpha': (1, 5),'reg_lambda': (0, 0.1)})\nxgboostBO.maximize()\nprint('-'*53)\nprint('Final Results')\nprint('XGBOOST: %f' % xgboostBO.res['max']['max_val'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b67bc69ce55362d04677daedfbdb0b8ff401fd64"
      },
      "cell_type": "markdown",
      "source": "<a id ='section14'></a>"
    },
    {
      "metadata": {
        "_uuid": "328762c2af5b6632e3f8dbc412d29d453923f6b4"
      },
      "cell_type": "markdown",
      "source": "### Discussion Questions Answers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6db710f20b6dd3409d1b32feff6ec0ec07f308b6"
      },
      "cell_type": "markdown",
      "source": "Q.1:- Briefly describe the conceptual approach you chose! What are the trade-offs?\n> First part of the choosen model is a gradient boosting ensemble model because decision boundary between two different classes are not Linearly separable.only Non-linear model will perform better in this case.VotingClassifier using two different Gradient Boosting packages (Xgboost,LightGbm) is the first part of the solution.\nThe second part is the same VotingClassifier with xgboost and lightgbm upon the dataset obtained from Denoising Auto Encoder which is chosen because the original train-test distribution is same and test set have more records then trainset.\nThe final solution is the weighted Average of both the models where weights were decided based upon Cross validation score of Traiset.\nTrade-offs:- Have used only the Gradient Boosting models in the final submission this could cause higher correlation of models in the ensemble which can lead to overfitting the trainSet.\n\nQ.2:- What's the model performance? What is the complexity? Where are the bottlenecks?\n> Roc_Auc have choose to measure the model Performance.Accuracy can't be used because of Unbalanced class count in the dataset.So we are getting 64% Roc_Auc score for 3 fold cv for First model of Ensemble and 63% Accuracy on Second Model with weighted Average the Score went to 65%.\nBecause of Ensemble of 4 different predictive models It is difficult for the interpretability of the model.the Runtime also more than single model.\n\n\nQ.3:- If you had more time, what improvements would you make, and in what order of priority?\n> 1. More feature generation by feature interaction and removing the unuseful features by keeping the useful one.\n2. Data Imputation by using predictive modelling instead of Mode.Features like num18 have higher feature importance but have lot of Null values.so,carefully filling these null values can result in accuracy increase.\n3. Tune the Xgboost and Dae Models on different Parameter space.Running the Dae Network for More number of Epochs or changing the Network architecture can also help.\n4. Doing Two Level Stacking with different and diverse Models."
    },
    {
      "metadata": {
        "_uuid": "a96167f1f95a91358a90c221d5be3258a711a124"
      },
      "cell_type": "markdown",
      "source": "### ****END****"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "545c94899950aa6b2b3e1c88d7b145a200359a10"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}